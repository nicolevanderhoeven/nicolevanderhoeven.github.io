<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>grafana otel community call on Nicole van der Hoeven</title>
    <link>https://nicolevanderhoeven.github.io/tags/grafana-otel-community-call/</link>
    <description>Recent content in grafana otel community call on Nicole van der Hoeven</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>Nicole van der Hoeven</copyright>
    <lastBuildDate>Wed, 19 Nov 2025 14:48:59 +0100</lastBuildDate><atom:link href="https://nicolevanderhoeven.github.io/tags/grafana-otel-community-call/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Lightweight Opensource APM with OTel Demo (Grafana ‚ù§Ô∏è‚Äçüî• OpenTelemetry Community Call #2)</title>
      <link>https://nicolevanderhoeven.github.io/blog/20251119-gocc02-lightweight-opensource-apm-with-otel-demo/</link>
      <pubDate>Wed, 19 Nov 2025 14:48:59 +0100</pubDate>
      
      <guid>https://nicolevanderhoeven.github.io/blog/20251119-gocc02-lightweight-opensource-apm-with-otel-demo/</guid>
      <description>&lt;p&gt;This community call with my coworker Liudmila Molkova featured an interview with Cyrille Le Clerc from Grafana Labs, focusing on the OpenTelemetry demo as a practical example of a lightweight, open-source Application Performance Monitoring (APM) solution. Cyril explained the purpose and architecture of the OTel demo, highlighting its role as a realistic, complex microservices application that has become a de facto standard for observability vendors. The discussion explored the modern definition of APM, followed by a detailed walkthrough of a Grafana dashboard built on OTel data, covering RED metrics, alerting, trace correlation, and infrastructure monitoring. A significant portion of the conversation was dedicated to the role and challenges of integrating logs within an OpenTelemetry framework. The interview concluded with a look at the future of the OTel demo, including plans for AI observability and eBPF instrumentation.&lt;/p&gt;

&lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
  &lt;iframe src=&#34;https://www.youtube.com/embed/cX74hkdI1zM&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; allowfullscreen title=&#34;YouTube Video&#34;&gt;&lt;/iframe&gt;
&lt;/div&gt;

&lt;h2 id=&#34;transcript&#34;&gt;Transcript&lt;/h2&gt;
&lt;h3 id=&#34;introduction&#34;&gt;Introduction&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Liudmila:&lt;/strong&gt;&lt;br&gt;
Okay. Hi everyone. Welcome to the second Grafana and OpenTelemetry community call. This time we&amp;rsquo;re going to talk about the lightweight open-source APM and we will use the OpenTelemetry demo as our playground. We have an expert, Cyrille, who works at Grafana. Cyrille, do you want to introduce yourCyrillef?&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Cyrille:&lt;/strong&gt;&lt;br&gt;
Thank you very much, Liudmila. My name is Cyrille, and I am a product manager at Grafana Labs. I&amp;rsquo;ve been working on open-source products for 15 years, first on Jenkins for CI/CDs, then at Elastic on observability, and now at Grafana Labs, also on observability. In addition to my day job, I have been an open-source contributor for about 20 years, starting with Apache Java projects and then moving on to Jenkins integration with Maven and OpenTelemetry. Now, I focus more on the OpenTelemetry project, like the OpenTelemetry demo, and I contribute to OpenTelemetry Helm charts as well. At Grafana Labs, I am the product management lead in charge of OpenTelemetry and I cover other areas as well. [laughter]&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Nicole:&lt;/strong&gt;&lt;br&gt;
That&amp;rsquo;s great. It&amp;rsquo;s great to have you here with us. Also, great for my internet to act up just a few minutes before this. Anyway, hi! I‚Äôm Nicole van der Hoeven, a developer advocate at Grafana. I work with Cyrille over there, and I&amp;rsquo;m really excited to be talking about OpenTelemetry. I&amp;rsquo;m the newbie of all of us here. [laughter] I have used OpenTelemetry in some demo apps I&amp;rsquo;ve built. Most recently, I used it for instrumenting an AI app. I&amp;rsquo;m excited to ask Cyrille all my questions because when he says he likes OpenTelemetry, what he really means is he&amp;rsquo;s the guy at Grafana who&amp;rsquo;s always banging on that OpenTelemetry drum. He&amp;rsquo;s like, &amp;ldquo;Tell all the things!&amp;rdquo; [laughter]&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Liudmila:&lt;/strong&gt;&lt;br&gt;
It&amp;rsquo;s probably my turn. I&amp;rsquo;m Liudmila. I work on the OpenTelemetry project and I&amp;rsquo;m a technical committee member. To echo what Nicole said, nobody knows all of OpenTelemetry, and this call is where we will uncover areas that are new to each of us. Going forward, we&amp;rsquo;ll probably have more of these calls. So, with that, let‚Äôs talk about the OpenTelemetry demo. Cyrille, what is it? Why do we care?&lt;/p&gt;
&lt;h3 id=&#34;what-is-the-opentelemetry-demo&#34;&gt;What is the OpenTelemetry Demo?&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Cyrille:&lt;/strong&gt;&lt;br&gt;
So, the OpenTelemetry demo was born to showcase OpenTelemetry. OpenTelemetry was created in the context of cloud-native architecture. This demo is a cloud-native application‚Äîa distributed microservices application that is instrumented with OpenTelemetry. It&amp;rsquo;s maintained by the OpenTelemetry community and showcases all dimensions of modern monitoring or observability, including all telemetry types: traces, metrics, logs, and more at every layer. We started with monitoring the application layer and are now extending to show the infrastructure layer.&lt;/p&gt;
&lt;p&gt;We demo various technologies: Java, .NET, Node.js, and nearly all programming languages, including Rust. We also show how to instrument middleware and databases, such as Kafka, PostgreSQL, and Redis, all deployed on Docker Compose to make it easy to play with on your local desktop or deployed on Kubernetes to be more production-like.&lt;/p&gt;
&lt;p&gt;This is a complex demo that shows everything, and what&amp;rsquo;s interesting is that it is realistic and currently used by 40 observability vendors to demonstrate their solutions with the OpenTelemetry demo. So, when you get familiar with the OpenTelemetry demo, it‚Äôs really easy to evaluate different vendors quickly. It comes &amp;ldquo;batteries included,&amp;rdquo; with open-source monitoring and observability technologies like Prometheus, Jaeger, OpenSearch, and Grafana to showcase how to implement an observability solution with OpenTelemetry.&lt;/p&gt;
&lt;h3 id=&#34;exploring-the-application&#34;&gt;Exploring the Application&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Liudmila:&lt;/strong&gt;&lt;br&gt;
Thanks for the intro! So maybe we should take a quick look at what the application itCyrillef looks like to make it more concrete.&lt;/p&gt;
&lt;p&gt;I&amp;rsquo;m connected to a Kubernetes cluster, and I&amp;rsquo;m just port forwarding. This is the telescope shop we have in the OpenTelemetry demo. We can go and buy things, and now we have something in the cart that we can place an order for. Oh, actually, this application might not work as expected. Well, that&amp;rsquo;s why we need observability, right? If things don&amp;rsquo;t work, we need to figure out what&amp;rsquo;s broken and how to fix it.&lt;/p&gt;
&lt;p&gt;Before we move on, I just want to show that even though you only see the front-end web shop, it&amp;rsquo;s a relatively large application. It has quite a few services, databases, caches, a front-end proxy, and a log generator that consistently sends requests.&lt;/p&gt;
&lt;p&gt;We&amp;rsquo;re super lucky to have all the telemetry Cyrille talked about. We have metrics in Prometheus, traces in Jaeger, and logs in OpenSearch, all visible in Grafana as a single pane of glass that you can refer to. With this, we can start looking at the telemetry itCyrillef. But I wanted to check with Cyrille or Nicole if you have any additional thoughts or anything we should cover here.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Nicole:&lt;/strong&gt;&lt;br&gt;
No, I think you presented it well. It&amp;rsquo;s a very complex topology because we want to demo everything.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Cyrille:&lt;/strong&gt;&lt;br&gt;
Yeah, it&amp;rsquo;s almost complex by design. You could probably have made this simpler, but it&amp;rsquo;s supposed to approximate real applications that aren&amp;rsquo;t simple. That&amp;rsquo;s the problem with some of these other demo apps; they are so simple to deploy that they work because they&amp;rsquo;re monolithic or something like that. This one is much more of a polyglot architecture, involving microservices that represent the real challenges that people face in the field.&lt;/p&gt;
&lt;h3 id=&#34;application-performance-monitoring-apm&#34;&gt;Application Performance Monitoring (APM)&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Liudmila:&lt;/strong&gt;&lt;br&gt;
One great thing about this demo is that sometimes it&amp;rsquo;s hard to understand how someone onboarded an application to OpenTelemetry. The demo has a lot of information on what services are used, and it demonstrates what&amp;rsquo;s been done to instrument this application with OpenTelemetry, which can help anyone with their journey in the OpenTelemetry space. Okay, wonderful! Let‚Äôs change gears and start looking at the APM dashboard.&lt;/p&gt;
&lt;p&gt;Before we go there, let&amp;rsquo;s talk about what APM is for a second. Why do we care? What is it?&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Cyrille:&lt;/strong&gt;&lt;br&gt;
APM is a very old concept‚Äîover 15 years old. We‚Äôve had several generations of monitoring tools. APM stands for Application Performance Monitoring. I see APM as a monitoring and observability technology that helps you monitor your application and understand its health.&lt;/p&gt;
&lt;p&gt;When I think about it, the first thing that comes to mind is understanding if my service is healthy or unhealthy. In a distributed architecture, I need to see my service in the context of the connections between services, typically represented with a service map. If I find something unhealthy, I need to drill down to see the health of a service. This means switching back and forth between the service metrics, which are more aggregated, and the traces, which give indications of how business transactions are executed.&lt;/p&gt;
&lt;p&gt;I also need to access logs to verify how things really happen and if there are messages that help me understand the problem. This is how I understand my service. Additionally, my service is deployed on a runtime and middleware, so I need to be able to zoom lower to see how it&amp;rsquo;s behaving on the Kubernetes pod or Linux box.&lt;/p&gt;
&lt;p&gt;APM allows me to switch back and forth between all layers, zooming in and out all the time across all telemetry types. This is essential for understanding health and troubleshooting. Another dimension is detecting anomalies, so I can wake people up at night when something goes wrong. This is where I create alerts and notifications.&lt;/p&gt;
&lt;p&gt;In the past, with monoliths, we created many alerts, but now we adopt best practices with SLOs, observing workflows, and root cause analysis. This is my vision of APM and why I think it‚Äôs more important than ever.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Nicole:&lt;/strong&gt;&lt;br&gt;
When you talk about application performance management or monitoring, do you differentiate that from the wider field of observability? Some people say APM sounds outdated because it focuses on the application or just on performance, which is ambiguous.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Cyrille:&lt;/strong&gt;&lt;br&gt;
The monitoring landscape has evolved a lot. With cloud-native architecture, many things have changed, and we had to change our best practices. The best practices from 20 years ago in monoliths are not the same as today‚Äôs best practices for distributed cloud-native architectures.&lt;/p&gt;
&lt;p&gt;There were disruptions in the past by innovators who focused primarily on traces, and we learned a lot from that. But now I think the landscape is getting more stable. We recognize the value in metrics, traces, and logs, and we‚Äôve evolved our best practices to leverage the best of all three.&lt;/p&gt;
&lt;p&gt;Some people prefer to call it observability. I know there are nuances, but this is my current perspective.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Liudmila:&lt;/strong&gt;&lt;br&gt;
It&amp;rsquo;s good to synchronize on terms. It sounds like we are on the same page.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Cyrille:&lt;/strong&gt;&lt;br&gt;
Now, with microservices architecture, we focus more on alerting symptoms rather than causes, centering more on the health of the application and the service provided to users than on the infrastructure layer. So, application monitoring or observability is becoming the entry point for people in charge of keeping the systems running.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Nicole:&lt;/strong&gt;&lt;br&gt;
Why don‚Äôt we take a look at the dashboard and go through it?&lt;/p&gt;
&lt;h3 id=&#34;apm-dashboard-exploration&#34;&gt;APM Dashboard Exploration&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Cyrille:&lt;/strong&gt;&lt;br&gt;
We will cover different parts of the dashboard throughout the hour. Let‚Äôs start with the top row‚Äîthe red metrics. You can call them duration, error, and request rate.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Nicole:&lt;/strong&gt;&lt;br&gt;
I believe the red metrics were created by Tom Wilkerson, who still works at Grafana. I think it was looCyrilley based on the Google SRE book, even though they didn&amp;rsquo;t explicitly say ‚Äúred‚Äù there.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Cyrille:&lt;/strong&gt;&lt;br&gt;
When you think about any particular service, you would think about some well-known terminology, which might be service level indicators, or just your application. Service level indicators talk more about what&amp;rsquo;s important for the application, while red metrics focus more on the telemetry itCyrillef, like the raw data behind your SLI.&lt;/p&gt;
&lt;p&gt;One thing that stands out is the alerts. It looks pretty awesome to see that something violates the threshold we&amp;rsquo;ve set‚Äîprobably high latency. Let‚Äôs explore it. We see it&amp;rsquo;s been firing for 17 hours. That‚Äôs quite a duration.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Nicole:&lt;/strong&gt;&lt;br&gt;
That‚Äôs not so good. Let‚Äôs see the alert on the latency. The P95 is firing for a specific operation.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Cyrille:&lt;/strong&gt;&lt;br&gt;
We can look at this chart, which shows the latency around P99, which is like 18 milliseconds. Is it really a problem?&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Nicole:&lt;/strong&gt;&lt;br&gt;
No, it was for the demo. It&amp;rsquo;s very situational; it depends on your services. The focus was to show practitioners best practices for creating alerts on HTTP endpoints. Typically, you have two families of alerts: latency alerts and error rate alerts.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Cyrille:&lt;/strong&gt;&lt;br&gt;
We decided to take P95 as a commonly adopted indicator. Some take P99, but P95 is often more relevant. Something that&amp;rsquo;s perhaps not noted here is that we take a time interval of 5 minutes. We wanted to provide this out of the box because it relates to the data collection interval, which is typically every minute for hotel setups and quite common in Prometheus.&lt;/p&gt;
&lt;p&gt;Having a 5-minute window is quite good. These are best practices we worked on, identifying good labels to help practitioners succeed by easily routing alerts to the right people and having the right notification schemes.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Nicole:&lt;/strong&gt;&lt;br&gt;
Imagine if there was actually high latency. Now we can enable this view and look at the examples of operations with the worst performance‚Äîlike the outliers you would see here. We use Jaeger as a store for telemetry, and if it were any other store that supports tracing, you would see something similar.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Cyrille:&lt;/strong&gt;&lt;br&gt;
This brings us to the Jaeger UI. I don&amp;rsquo;t want to spend a lot of time here, but it‚Äôs just another confirmation that nothing is visibly wrong with this operation. Things are just going a bit slower in the cloud, as usual.&lt;/p&gt;
&lt;h4 id=&#34;alert-analysis&#34;&gt;Alert Analysis&lt;/h4&gt;
&lt;p&gt;&lt;strong&gt;Liudmila:&lt;/strong&gt;&lt;br&gt;
We talked about alerts. There are other interesting things in this picture. For example, the request rate is pretty normal. If there were a spike in usage, we would see it here. The error rate looks good‚Äîthere are no errors. By looking at these three metrics, you can get a basic understanding of the health of your system and this specific service.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Cyrille:&lt;/strong&gt;&lt;br&gt;
This dashboard illustrates years of best practices that the industry has accumulated. We have layers of information. At the top, you have your service identifier, the last metrics received time, and the overview of the aggregated service red metrics.&lt;/p&gt;
&lt;p&gt;This aggregated view across all operations of my service will help you decide if this service is healthy or not. If you have doubts or think something is going wrong, P95 is good at surfacing outliers.&lt;/p&gt;
&lt;p&gt;Typically, you will scroll down to the HTTP or gRPC operations, where you have the details broken down by operation. It‚Äôs likely that not all business operations have problems; it could be just a subset of them.&lt;/p&gt;
&lt;p&gt;Here you can see the outbound services. If you have a problem, this data will help you understand if it‚Äôs happening within your service or is it a downstream service. In microservices architecture, that‚Äôs always a question.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Nicole:&lt;/strong&gt;&lt;br&gt;
Are those dashboards powered exclusively by metrics?&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Cyrille:&lt;/strong&gt;&lt;br&gt;
Everything is powered by metrics, specifically the OpenTelemetry semantic conventions. Initially, the OpenTelemetry demo derived metrics from spans, which was a great idea four years ago, but it had some limits. Now, OpenTelemetry has standardized some semantic conventions for HTTP server calls, client calls, database client calls, RPC calls, and messaging.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Nicole:&lt;/strong&gt;&lt;br&gt;
I noticed during my time with the demo that the coverage of metrics is uneven across programming languages. Java, for instance, has great instrumentation, while in other languages, it depends on the instrumentation.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Cyrille:&lt;/strong&gt;&lt;br&gt;
Yes, that‚Äôs true. It‚Äôs uncommon for OpenTelemetry instrumentations to emit logs these days, but let‚Äôs move on to that.&lt;/p&gt;
&lt;h3 id=&#34;logs-and-observability&#34;&gt;Logs and Observability&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Nicole:&lt;/strong&gt;&lt;br&gt;
So, what‚Äôs the deal with logs? Why are they not as important?&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Cyrille:&lt;/strong&gt;&lt;br&gt;
Logs are traditionally outputted in stdout, and we‚Äôve lost Nicole. [laughter] The challenge is that traces and metrics are captured by SDKs within your app, while logs come from external log collectors, making correlation tough. When we discuss with practitioners and tell them that the OpenTelemetry SDK can directly export logs, they often respond that their log processes are mature and they can‚Äôt disrupt them just to trust the new kid on the block.&lt;/p&gt;
&lt;p&gt;It takes time to change your log toolchain. And while OpenTelemetry SDKs have capabilities to directly export logs, it‚Äôs not easy to get people to change practices they‚Äôve had for 30 years.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Nicole:&lt;/strong&gt;&lt;br&gt;
To demonstrate Cyrille‚Äôs point about correlation, the most basic way to correlate is by trace ID and span ID. When we capture distributed traces, we can also attribute logs to the specific operation.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Cyrille:&lt;/strong&gt;&lt;br&gt;
Yes, and when we look at a trace inside Grafana, we can see logs associated with it.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Nicole:&lt;/strong&gt;&lt;br&gt;
If we open the trace view, we can see the details for this operation. There are a bunch of attributes you can correlate to understand what happened.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Cyrille:&lt;/strong&gt;&lt;br&gt;
Logs provide the details. I think the negative sentiment around logs came from the idea that they don‚Äôt work as well in distributed applications as they did in monolithic applications. We had to evolve our practices to log the right way for distributed applications.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Nicole:&lt;/strong&gt;&lt;br&gt;
It‚Äôs user error because logs are the signals you can abuse the most. You can put anything in a log line, which causes issues. With metrics, you don‚Äôt have as much leeway, which makes it simpler to deal with.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Cyrille:&lt;/strong&gt;&lt;br&gt;
Yes, and we need to convince users that going into log analytics tools is much more efficient than tailing logs directly.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Nicole:&lt;/strong&gt;&lt;br&gt;
It is more complicated to analyze logs than traces because they aren‚Äôt always structured.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Cyrille:&lt;/strong&gt;&lt;br&gt;
Exactly. We have to help organizations better structure their logs for better value.&lt;/p&gt;
&lt;h3 id=&#34;questions-and-community-engagement&#34;&gt;Questions and Community Engagement&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Liudmila:&lt;/strong&gt;&lt;br&gt;
We have a question here. ‚ÄúWe are planning to adopt the LGTM stack as an observability platform. Is there any blueprint documentation aligned with observability 2.0?‚Äù&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Cyrille:&lt;/strong&gt;&lt;br&gt;
LGTM stands for Loki, Grafana, Tempo, and Mimir. It‚Äôs an integrated solution. Observability 2.0 is about unified telemetry‚Äîhaving everything correlated and not just reacting to errors. It‚Äôs about understanding business impacts when you decrease response times.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Nicole:&lt;/strong&gt;&lt;br&gt;
So, it‚Äôs more about understanding the overall system rather than just monitoring.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Cyrille:&lt;/strong&gt;&lt;br&gt;
Yes, exactly.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Liudmila:&lt;/strong&gt;&lt;br&gt;
How do I know which metrics and traces to prioritize first when instrumenting a distributed system with OpenTelemetry?&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Cyrille:&lt;/strong&gt;&lt;br&gt;
We recommend starting with auto-instrumentation first. It provides metrics and traces at a technical level, and with HTTP, the endpoints are meaningful from a business standpoint. You start with auto-instrumentation and then add custom instrumentation on critical workflows.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Nicole:&lt;/strong&gt;&lt;br&gt;
What‚Äôs the current state of the OpenTelemetry demo? Can it monitor infrastructure as well?&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Cyrille:&lt;/strong&gt;&lt;br&gt;
Yes, OpenTelemetry was born in the application layer, but infrastructure monitoring came a bit later. We‚Äôre working with the hotel demo folks to showcase better infrastructure monitoring done by OpenTelemetry.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Liudmila:&lt;/strong&gt;&lt;br&gt;
If people want to contribute or try out the hotel demo for themCyrilleves, where should they go?&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Nicole:&lt;/strong&gt;&lt;br&gt;
I posted the link in the chat to the OpenTelemetry site.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Cyrille:&lt;/strong&gt;&lt;br&gt;
If you want to participate in the demo, we have community meetings, and I can share another link in the channel for more information.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Liudmila:&lt;/strong&gt;&lt;br&gt;
Great! Thank you both for being here and for everyone watching. I think that&amp;rsquo;s a great way to end this call.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Cyrille:&lt;/strong&gt;&lt;br&gt;
Thank you very much! It was a pleasure to be invited and an honor.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Nicole:&lt;/strong&gt;&lt;br&gt;
Thank you!&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Declarative Configuration in OpenTelemetry SDKs (Grafana ‚ù§Ô∏è‚Äçüî• OpenTelemetry Community Call October 2025)</title>
      <link>https://nicolevanderhoeven.github.io/blog/20251022-gocc01-declarative-config-in-otel/</link>
      <pubDate>Wed, 22 Oct 2025 15:02:21 -0700</pubDate>
      
      <guid>https://nicolevanderhoeven.github.io/blog/20251022-gocc01-declarative-config-in-otel/</guid>
      <description>&lt;p&gt;I&amp;rsquo;m very lucky to have the opportunity to work with &lt;a href=&#34;https://www.linkedin.com/in/lmolkova/&#34;&gt;Liudmila Molkova&lt;/a&gt;, who just joined as a Staff Developer Advocate at Grafana Labs. We decided we would start a new monthly livestream called the Grafana ‚ù§Ô∏è‚Äçüî• OpenTelemetry Community Call. In this inaugural episode, we interviewed &lt;a href=&#34;https://www.linkedin.com/in/maryliag/&#34;&gt;Marylia Gutierrez&lt;/a&gt; about declarative configuration for OpenTelemetry SDKs. We talked about:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Pains in getting started with OpenTelemetry, particularly around instrumentation and configuration&lt;/li&gt;
&lt;li&gt;What declarative configuration is, and how it improves the experience&lt;/li&gt;
&lt;li&gt;Which languages declarative configuration is for&lt;/li&gt;
&lt;li&gt;The future of declarative configuration in OpenTelemetry&lt;/li&gt;
&lt;li&gt;Marylia&amp;rsquo;s candidacy for Governing Committee of OTel (&lt;a href=&#34;https://gra.fan/otelvote&#34;&gt;vote for her here&lt;/a&gt; from October 27th-30th, 2025)&lt;/li&gt;
&lt;/ul&gt;

&lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
  &lt;iframe src=&#34;https://www.youtube.com/embed/FwnyV5z_ij4&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; allowfullscreen title=&#34;YouTube Video&#34;&gt;&lt;/iframe&gt;
&lt;/div&gt;

</description>
    </item>
    
  </channel>
</rss>